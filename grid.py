# ç½‘æ ¼æœç´¢Î±Î²Î³.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import numpy as np
import pandas as pd
import os
import logging
from tqdm import tqdm
import matplotlib.pyplot as plt
from wifi_localization.config import Config
from wifi_localization.data.preprocessing import load_and_preprocess_data
from wifi_localization.models.base_model import WiFiLocalizationModel
from wifi_localization.training.trainer import train_model, validate_epoch
# è®¾ç½®è®¾å¤‡
assert torch.cuda.is_available(), "CUDA is not available - GPU required!"
device = torch.device('cuda')
print(f"Using device: {device}")

def compute_composite_score(building_acc, floor_acc, coord_mae):
    """
    è®¡ç®—ç»¼åˆè¯„åˆ†
    - å»ºç­‘å’Œæ¥¼å±‚å‡†ç¡®ç‡è¶Šé«˜è¶Šå¥½
    - åæ ‡è¯¯å·®è¶Šå°è¶Šå¥½
    """
    # å°†åæ ‡MAEè½¬æ¢ä¸ºå¾—åˆ†ï¼ˆè¯¯å·®è¶Šå°å¾—åˆ†è¶Šé«˜ï¼‰
    coord_score = 1.0 / (coord_mae + 1e-8)
    coord_score = min(coord_score, 0.5)  # é™åˆ¶æœ€å¤§å¾—åˆ†

    # åŠ æƒç»¼åˆå¾—åˆ†
    composite_score = (0.2 * building_acc +  # å»ºç­‘åˆ†ç±»æƒé‡20%
                       0.5 * floor_acc +  # æ¥¼å±‚åˆ†ç±»æƒé‡50%
                       0.3 * coord_score)  # åæ ‡å›å½’æƒé‡30%

    return composite_score


def grid_search_weights():
    """æ‰§è¡ŒæŸå¤±æƒé‡ç½‘æ ¼æœç´¢"""

    # 1. åŠ è½½æ•°æ®
    TRAIN_PATH = 'TrainingData.csv'
    VAL_PATH = 'ValidationData.csv'

    train_loader, val_loader, num_buildings, num_floors, floor_counts, coord_mean, coord_std = load_and_preprocess_data(
        train_data_path=TRAIN_PATH,
        val_data_path=VAL_PATH,
        train_subset_ratio=1.0,
        device=device
    )

    # 2. å‡†å¤‡åŸºç¡€é…ç½®
    model_config = Config.MODEL_CONFIG.copy()
    model_config.update({
        'num_buildings': num_buildings,
        'num_floors': num_floors,
        'num_classes': num_buildings + num_floors
    })

    print(f"æ¨¡å‹é…ç½®: å»ºç­‘={num_buildings}, æ¥¼å±‚={num_floors}")

    # 3. å®šä¹‰è¦æœç´¢çš„æƒé‡ç»„åˆ
    weight_configs = [
        (0.1, 1.0, 0.5),  # ç»„åˆ1ï¼šå¼ºè°ƒæ¥¼å±‚åˆ†ç±»
        (0.3, 1.0, 0.7),  # ç»„åˆ2ï¼šåŸå§‹è®¾ç½®
        (0.5, 1.0, 1.0),  # ç»„åˆ3ï¼šå‡è¡¡æƒé‡
        (0.2, 1.0, 0.8),  # ç»„åˆ4ï¼šé€‚åº¦å¼ºè°ƒåæ ‡
        (0.1, 0.8, 1.0),  # ç»„åˆ5ï¼šå¼ºè°ƒåæ ‡å›å½’
        (0.4, 1.0, 0.6),  # ç»„åˆ6ï¼šå»ºç­‘æƒé‡ç¨é«˜
    ]

    # 4. ç½‘æ ¼æœç´¢
    best_composite_score = 0
    best_weights = None
    all_results = []

    for i, weights in enumerate(weight_configs):
        print(f"\n{'=' * 50}")
        print(f"ğŸ”¬ æµ‹è¯•æƒé‡ç»„åˆ {i + 1}/{len(weight_configs)}: Î±={weights[0]}, Î²={weights[1]}, Î³={weights[2]}")
        print(f"{'=' * 50}")

        model = WiFiLocalizationModel(model_config)

        # å‡†å¤‡æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        floor_weights_tensor = torch.tensor(1.0 / (floor_counts + 1e-7), dtype=torch.float32).to(device)
        floor_loss_fn = nn.CrossEntropyLoss(weight=floor_weights_tensor, label_smoothing=0.05)

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=1e-2)
        scheduler = StepLR(optimizer, step_size=15, gamma=0.5)

        # ä½¿ç”¨ç®€åŒ–çš„è®­ç»ƒï¼ˆå‡å°‘epochsä»¥åŠ é€Ÿæœç´¢ï¼‰
        train_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            criterion=criterion,
            floor_loss_fn=floor_loss_fn,
            optimizer=optimizer,
            scheduler=scheduler,
            coord_mean=coord_mean,
            coord_std=coord_std,
            loss_weights=weights,
            flood_level=Config.FLOOD_LEVEL,
            device=device,
            save_plots=False  # ä¸ä¿å­˜æ¯ä¸ªç»„åˆçš„å›¾è¡¨
        )

        # è¯„ä¼°å½“å‰æƒé‡ç»„åˆçš„æ€§èƒ½
        final_metrics = validate_epoch(
            model, val_loader, criterion, floor_loss_fn, device, coord_mean, coord_std
        )

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        composite_score = compute_composite_score(
            final_metrics['building_acc'],
            final_metrics['floor_acc'],
            final_metrics['coord_mae']
        )

        # è®°å½•ç»“æœ
        result = {
            'weights': weights,
            'composite_score': composite_score,
            'building_acc': final_metrics['building_acc'],
            'floor_acc': final_metrics['floor_acc'],
            'coord_mae': final_metrics['coord_mae'],
            'val_loss': final_metrics['loss']
        }
        all_results.append(result)

        print(f"âœ… ç»„åˆç»“æœ:")
        print(f"   å»ºç­‘å‡†ç¡®ç‡: {result['building_acc']:.4f}")
        print(f"   æ¥¼å±‚å‡†ç¡®ç‡: {result['floor_acc']:.4f}")
        print(f"   åæ ‡MAE: {result['coord_mae']:.2f}m")
        print(f"   ç»¼åˆè¯„åˆ†: {result['composite_score']:.4f}")

        # æ›´æ–°æœ€ä½³æƒé‡
        if composite_score > best_composite_score:
            best_composite_score = composite_score
            best_weights = weights
            print(f"ğŸ† æ–°çš„æœ€ä½³æƒé‡: {best_weights}")

    # 5. è¾“å‡ºç½‘æ ¼æœç´¢æ€»ç»“
    print(f"\n{'=' * 60}")
    print("ğŸ¯ ç½‘æ ¼æœç´¢å®Œæˆï¼ç»“æœæ€»ç»“:")
    print(f"{'=' * 60}")

    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
    all_results.sort(key=lambda x: x['composite_score'], reverse=True)

    for i, result in enumerate(all_results):
        Î±, Î², Î³ = result['weights']
        rank = f"{i + 1}." if i < 3 else "   "
        print(f"{rank} æƒé‡(Î±={Î±}, Î²={Î²}, Î³={Î³}): "
              f"å»ºç­‘={result['building_acc']:.4f}, "
              f"æ¥¼å±‚={result['floor_acc']:.4f}, "
              f"MAE={result['coord_mae']:.2f}m, "
              f"ç»¼åˆ={result['composite_score']:.4f}")

    # 6. ä½¿ç”¨æœ€ä½³æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ
    print(f"\n{'=' * 50}")
    print(f"ğŸš€ ä½¿ç”¨æœ€ä½³æƒé‡è¿›è¡Œæœ€ç»ˆè®­ç»ƒ: Î±={best_weights[0]}, Î²={best_weights[1]}, Î³={best_weights[2]}")
    print(f"{'=' * 50}")

    final_model = WiFiLocalizationModel(model_config)
    final_optimizer = optim.Adam(final_model.parameters(), lr=Config.LEARNING_RATE, weight_decay=1e-2)
    final_scheduler = StepLR(final_optimizer, step_size=15, gamma=0.5)

    # å®Œæ•´è®­ç»ƒ
    train_model(
        model=final_model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        floor_loss_fn=floor_loss_fn,
        optimizer=final_optimizer,
        scheduler=final_scheduler,
        num_epochs=Config.NUM_EPOCHS,
        patience=Config.PATIENCE,
        coord_mean=coord_mean,
        coord_std=coord_std,
        loss_weights=best_weights,
        flood_level=Config.FLOOD_LEVEL,
        device=device
    )

    return best_weights, all_results


def visualize_grid_results(all_results):
    """å¯è§†åŒ–ç½‘æ ¼æœç´¢ç»“æœ"""

    # æå–æ•°æ®
    alphas = [r['weights'][0] for r in all_results]
    betas = [r['weights'][1] for r in all_results]
    gammas = [r['weights'][2] for r in all_results]
    composite_scores = [r['composite_score'] for r in all_results]
    building_accs = [r['building_acc'] for r in all_results]
    floor_accs = [r['floor_acc'] for r in all_results]
    coord_maes = [r['coord_mae'] for r in all_results]

    # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
    unique_alphas = sorted(set(alphas))
    unique_gammas = sorted(set(gammas))
    heatmap_data = np.zeros((len(unique_alphas), len(unique_gammas)))

    for i, alpha in enumerate(unique_alphas):
        for j, gamma in enumerate(unique_gammas):
            # æ‰¾åˆ°å¯¹åº”çš„ç»“æœï¼ˆå‡è®¾betaå›ºå®šä¸º1.0ï¼‰
            for result in all_results:
                if abs(result['weights'][0] - alpha) < 0.01 and abs(result['weights'][2] - gamma) < 0.01:
                    heatmap_data[i, j] = result['composite_score']
                    break

    plt.figure(figsize=(10, 8))
    plt.imshow(heatmap_data, cmap='viridis', aspect='auto')
    plt.colorbar(label='ç»¼åˆè¯„åˆ†')
    plt.xticks(range(len(unique_gammas)), [f'{g:.1f}' for g in unique_gammas])
    plt.yticks(range(len(unique_alphas)), [f'{a:.1f}' for a in unique_alphas])
    plt.xlabel('Î³ (åæ ‡æƒé‡)')
    plt.ylabel('Î± (å»ºç­‘æƒé‡)')
    plt.title('æŸå¤±æƒé‡ç½‘æ ¼æœç´¢çƒ­åŠ›å›¾ (Î²=1.0å›ºå®š)')
    plt.tight_layout()
    plt.savefig('grid_search_heatmap.png', dpi=300)
    print("âœ… ç½‘æ ¼æœç´¢çƒ­åŠ›å›¾å·²ä¿å­˜ä¸º grid_search_heatmap.png")

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    axes[0, 0].bar(range(len(composite_scores)), composite_scores)
    axes[0, 0].set_xlabel('æƒé‡ç»„åˆ')
    axes[0, 0].set_ylabel('ç»¼åˆè¯„åˆ†')
    axes[0, 0].set_title('å„æƒé‡ç»„åˆç»¼åˆè¯„åˆ†')
    axes[0, 0].set_xticks(range(len(composite_scores)))

    x = range(len(all_results))
    axes[0, 1].plot(x, building_accs, 'o-', label='å»ºç­‘å‡†ç¡®ç‡')
    axes[0, 1].plot(x, floor_accs, 's-', label='æ¥¼å±‚å‡†ç¡®ç‡')
    axes[0, 1].set_xlabel('æƒé‡ç»„åˆ')
    axes[0, 1].set_ylabel('å‡†ç¡®ç‡')
    axes[0, 1].set_title('åˆ†ç±»å‡†ç¡®ç‡å¯¹æ¯”')
    axes[0, 1].legend()
    axes[0, 1].set_xticks(x)

    axes[1, 0].bar(x, coord_maes)
    axes[1, 0].set_xlabel('æƒé‡ç»„åˆ')
    axes[1, 0].set_ylabel('MAE (ç±³)')
    axes[1, 0].set_title('åæ ‡å®šä½è¯¯å·®å¯¹æ¯”')
    axes[1, 0].set_xticks(x)

    scatter = axes[1, 1].scatter(alphas, gammas, c=composite_scores, s=100, cmap='viridis')
    axes[1, 1].set_xlabel('Î± (å»ºç­‘æƒé‡)')
    axes[1, 1].set_ylabel('Î³ (åæ ‡æƒé‡)')
    axes[1, 1].set_title('æƒé‡å‚æ•°åˆ†å¸ƒ')
    plt.colorbar(scatter, ax=axes[1, 1], label='ç»¼åˆè¯„åˆ†')

    plt.tight_layout()
    plt.savefig('grid_search_analysis.png', dpi=300)
    print("âœ… ç½‘æ ¼æœç´¢ç»“æœåˆ†æå›¾å·²ä¿å­˜ä¸º grid_search_analysis.png")

    plt.close('all')


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æŸå¤±æƒé‡ç½‘æ ¼æœç´¢")
    print("=" * 60)

    try:
        # æ‰§è¡Œç½‘æ ¼æœç´¢
        best_weights, all_results = grid_search_weights()

        # å¯è§†åŒ–ç»“æœ
        visualize_grid_results(all_results)

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        results_df = pd.DataFrame(all_results)
        results_df.to_csv('grid_search_results.csv', index=False)
        print("âœ… ç½‘æ ¼æœç´¢ç»“æœå·²ä¿å­˜åˆ° grid_search_results.csv")

        print(f"\nğŸ¯ æœ€ä½³æƒé‡ç»„åˆ: Î±={best_weights[0]}, Î²={best_weights[1]}, Î³={best_weights[2]}")

    except Exception as e:
        logger.error(f"ç½‘æ ¼æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
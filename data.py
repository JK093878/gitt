# data_preprocessing.py
import pandas as pd
import numpy as np
import torch
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from torch.utils.data import DataLoader, Subset
import os

from utils import augment_rssi_with_noise
from models.base_models import train_gan_rssi, train_dae, apply_dae
from data_loader import WiFiDataset


def load_and_preprocess_data(train_data_path, val_data_path, seq_length=5,
                             train_subset_ratio=1.0, device='cuda'):
    # è·¯å¾„æ£€æŸ¥
    if not os.path.exists(train_data_path):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{train_data_path}")
    if not os.path.exists(val_data_path):
        raise FileNotFoundError(f"éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{val_data_path}")

    # åŠ è½½æ•°æ®
    train_data = pd.read_csv(train_data_path)
    val_data = pd.read_csv(val_data_path)

    # å¢å¼º Floor 4 æ ·æœ¬
    print("ğŸ” å¼€å§‹å¢å¼º Floor 4 æ ·æœ¬")
    train_data = augment_floor4_samples(train_data, device)

    # æ ‡ç­¾ä¿®æ­£
    train_data, val_data = adjust_labels(train_data, val_data)

    # é¢„å¤„ç†åæ ‡
    coord_train, coord_val, coord_mean, coord_std = preprocess_coordinates(train_data, val_data)

    # è®­ç»ƒå’Œåº”ç”¨DAE
    print("Training DAE model...")
    dae = train_dae(
        features=train_data.filter(regex='^WAP').values,
        device=device
    )

    print("Applying DAE to train/val data...")
    denoised_train_features = apply_dae(
        dae,
        features=train_data.filter(regex='^WAP').values,
        device=device
    )

    denoised_val_features = apply_dae(
        dae,
        features=val_data.filter(regex='^WAP').values,
        device=device
    )

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = WiFiDataset(
        features=denoised_train_features,
        labels=train_data['FLOOR'].values,
        building_ids=train_data['BUILDINGID'].values,
        coordinates=coord_train,
        seq_length=seq_length,
        is_train=True
    )

    val_dataset = WiFiDataset(
        features=denoised_val_features,
        labels=val_data['FLOOR'].values,
        building_ids=val_data['BUILDINGID'].values,
        coordinates=coord_val,
        seq_length=seq_length,
        imputer=train_dataset.imputer,
        scaler=train_dataset.scaler,
        is_train=False
    )

    if train_subset_ratio < 1.0:
        subset_size = int(train_subset_ratio * len(train_dataset))
        train_dataset = Subset(train_dataset, range(subset_size))

    batch_size = 128
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # è®¡ç®—floor_counts
    if isinstance(train_loader.dataset, Subset):
        floor_counts = np.bincount(train_loader.dataset.dataset.labels)
    else:
        floor_counts = np.bincount(train_loader.dataset.labels)

    return train_loader, val_loader, num_buildings, num_floors, floor_counts, coord_mean, coord_std


def augment_floor4_samples(train_data, device):
    """å¢å¼ºFloor 4æ ·æœ¬"""
    floor4_data = train_data[train_data['FLOOR'] == 4].copy()
    if len(floor4_data) == 0:
        return train_data

    floor4_rssi = floor4_data.filter(regex='^WAP').values
    floor4_rssi[floor4_rssi == 100] = -104
    floor4_rssi = (floor4_rssi + 104) / 104 * 2 - 1

    # è®­ç»ƒGAN
    rssi_gan = train_gan_rssi(floor4_rssi, num_epochs=100, device=device)

    # ç”Ÿæˆåˆæˆæ ·æœ¬
    num_to_generate = 2000
    z = torch.randn(num_to_generate, 100).to(device)
    synthetic_rssi = rssi_gan(z).detach().cpu().numpy()
    synthetic_rssi = (synthetic_rssi + 1) / 2 * 104 - 104
    synthetic_rssi = np.clip(synthetic_rssi, -104, 0)

    synthetic_df = pd.DataFrame(synthetic_rssi, columns=train_data.filter(regex='^WAP').columns)
    synthetic_df['FLOOR'] = 4
    synthetic_df['BUILDINGID'] = 2

    # ä½¿ç”¨KMeansåˆ†é…åæ ‡
    synthetic_df = assign_coordinates_to_synthetic(synthetic_df, floor4_data)

    # åˆå¹¶åˆ°è®­ç»ƒé›†
    train_data = pd.concat([train_data, synthetic_df], ignore_index=True)
    print(f"âœ… Floor 4 æ ·æœ¬å¢å¼ºåæ•°é‡: {len(train_data[train_data['FLOOR'] == 4])}")

    return train_data


def assign_coordinates_to_synthetic(synthetic_df, floor4_data):
    """ä¸ºåˆæˆæ ·æœ¬åˆ†é…åæ ‡"""
    num_clusters = 4
    kmeans = KMeans(n_clusters=4, n_init=10, random_state=42)
    coords = floor4_data[['LONGITUDE', 'LATITUDE']].values
    cluster_labels = kmeans.fit_predict(coords)

    samples_per_cluster = len(synthetic_df) // num_clusters
    synthetic_parts = []

    for i in range(num_clusters):
        cluster_mask = cluster_labels == i
        cluster_coords = coords[cluster_mask]
        if len(cluster_coords) < 2:
            continue

        lon_mean, lon_std = cluster_coords[:, 0].mean(), cluster_coords[:, 0].std()
        lat_mean, lat_std = cluster_coords[:, 1].mean(), cluster_coords[:, 1].std()

        synth_part = synthetic_df.iloc[i * samples_per_cluster:(i + 1) * samples_per_cluster].copy()
        synth_part['LONGITUDE'] = np.random.normal(lon_mean, lon_std, size=len(synth_part))
        synth_part['LATITUDE'] = np.random.normal(lat_mean, lat_std, size=len(synth_part))
        synthetic_parts.append(synth_part)

    return pd.concat(synthetic_parts, ignore_index=True)


def adjust_labels(train_data, val_data):
    """è°ƒæ•´æ ‡ç­¾ä»0å¼€å§‹"""
    for col in ['BUILDINGID', 'FLOOR']:
        train_min = train_data[col].min()
        train_data[col] -= train_min
        val_data[col] -= train_min

    # éªŒè¯
    for data, name in [(train_data, 'è®­ç»ƒé›†'), (val_data, 'éªŒè¯é›†')]:
        print(f"\n{name}æ ‡ç­¾èŒƒå›´:")
        print(f"Building IDèŒƒå›´: {data['BUILDINGID'].min()} - {data['BUILDINGID'].max()}")
        print(f"Floor IDèŒƒå›´: {data['FLOOR'].min()} - {data['FLOOR'].max()}")

    num_buildings = int(train_data['BUILDINGID'].max()) + 1
    num_floors = int(train_data['FLOOR'].max()) + 1

    print(f"\næ¨¡å‹é…ç½®:")
    print(f"å»ºç­‘ç‰©æ•°é‡: {num_buildings}")
    print(f"æ¥¼å±‚æ•°é‡: {num_floors}")

    return train_data, val_data


def preprocess_coordinates(train_data, val_data):
    """é¢„å¤„ç†åæ ‡æ•°æ®"""
    coord_train = train_data[['LONGITUDE', 'LATITUDE']].values
    coord_val = val_data[['LONGITUDE', 'LATITUDE']].values

    coord_mean = coord_train.mean(axis=0)
    coord_std = coord_train.std(axis=0)

    coord_train = (coord_train - coord_mean) / coord_std
    coord_val = (coord_val - coord_mean) / coord_std

    return coord_train, coord_val, coord_mean, coord_std